{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Universidad de Buenos Aires\n",
        "# Aprendizaje Profundo - TP3\n",
        "# Cohorte 22 - 5to bimestre 2025\n"
      ],
      "metadata": {
        "id": "tHbzg4F1fLo7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este tercer y último TP se debe entregar hasta las **23hs del viernes 12 de diciembre (hora de Argentina)**. La resolución del TP es **individual**. Pueden utilizar los contenidos vistos en clase y otra bibliografía. Si se toman ideas de fuentes externas deben ser correctamente citadas incluyendo el correspondiente link o página de libro.\n",
        "\n",
        "ESTE TP3 EQUIVALE A UN TERCIO DE SU NOTA FINAL.\n",
        "\n",
        "El formato de entrega debe ser un link a un notebook de google colab. Permitir acceso a gvilcamiza.ext@fi.uba.ar y **habilitar los comentarios, para poder darles el feedback**. Si no lo hacen así no se podrá dar el feedback respectivo por cada pregunta.\n",
        "\n",
        "El envío **se realizará en el siguiente link de google forms: [link](https://forms.gle/kscHDArwzdvrTSG99)**. Tanto los resultados, gráficas, como el código y las explicaciones deben quedar guardados y visualizables en el colab.\n",
        "\n",
        "**NO SE VALIDARÁN ENVÍOS POR CORREO, EL MÉTODO DE ENTREGA ES SOLO POR EL FORMS.**\n",
        "\n",
        "**Consideraciones a tener en cuenta:**\n",
        "- Se entregará 1 solo colab para este TP3.\n",
        "- Renombrar el archivo de la siguiente manera: **APELLIDO-NOMBRE-DL-TP3-Co22.ipynb**\n",
        "- Los códigos deben poder ejecutarse.\n",
        "- Los resultados, cómo el código, los gráficos y las explicaciones deben quedar guardados y visualizables en el correspondiente notebook.\n",
        "- Prestar atención a las consignas, responder las preguntas cuando corresponda.\n",
        "- Solo se revisarán los trabajos que hayan sido enviados por el forms."
      ],
      "metadata": {
        "id": "PEib4WVwfQYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CLASIFICADOR DE EMOCIONES**"
      ],
      "metadata": {
        "id": "bdseNqG3m7xX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo de este trabajo es construir una red neuronal convolucional (CNN) utilizando Pytorch, capaz de clasificar emociones humanas a partir de imágenes faciales. El clasificador deberá identificar una de las 7 emociones básicas: alegría, tristeza, enojo, miedo, sorpresa, disgusto y seriedad. El dataset se encuentra en este link: https://drive.google.com/file/d/1aPHE00zkDhEV1waJKhaOJMdN6-lUc0iT/view?usp=sharing"
      ],
      "metadata": {
        "id": "u8jyqDP8bom6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les recomiendo usar el siguiente código para poder obtener las imágenes fácilmente desde ese link. Pero son libres de descargar las imágenes como mejor les parezca."
      ],
      "metadata": {
        "id": "xFE2KIAqseS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=1aPHE00zkDhEV1waJKhaOJMdN6-lUc0iT\"\n",
        "output = \"archivo.zip\"\n",
        "\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "destino = \"datos_zip\"\n",
        "os.makedirs(destino, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "    zip_ref.extractall(destino)\n",
        "\n",
        "DATASET_ROOT_TRAIN = '/content/datos_zip/dataset_emociones/train'\n",
        "DATASET_ROOT_VAL   = '/content/datos_zip/dataset_emociones/validation'\n"
      ],
      "metadata": {
        "id": "an8TgypOsdol",
        "outputId": "6e26b9b1-524b-4c0c-d413-1da46aef10b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1aPHE00zkDhEV1waJKhaOJMdN6-lUc0iT\n",
            "From (redirected): https://drive.google.com/uc?id=1aPHE00zkDhEV1waJKhaOJMdN6-lUc0iT&confirm=t&uuid=74bfab4a-fc04-4aed-a627-c1929ee581b0\n",
            "To: /content/archivo.zip\n",
            "100%|██████████| 40.0M/40.0M [00:00<00:00, 85.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Preprocesamiento de Datos (2 puntos)\n",
        "\n",
        "Antes de entrenar el modelo, se debe analizar qué tipo de preprocesamiento se debe aplicar a las imágenes. Para esto, se puede considerar uno o más aspectos como:\n",
        "\n",
        "- Tamaño\n",
        "- Relación de aspecto\n",
        "- Color o escala de grises\n",
        "- Cambio de dimensionalidad\n",
        "- Normalización\n",
        "- Balanceo de datos\n",
        "- Data augmentation\n",
        "- etc.\n",
        "\n",
        "Sean criteriosos y elijan solo las técnicas que consideren pertinentes para este caso de uso en específico.\n",
        "\n",
        "Recomendación: usar `torchvision.transforms` para facilitar el preprocesamiento. Lean su documentación si tienen dudas: https://docs.pytorch.org/vision/0.14/transforms.html\n",
        "\n"
      ],
      "metadata": {
        "id": "Y-ouGrVnbp7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    root=str(DATASET_ROOT_TRAIN),\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "val_dataset = datasets.ImageFolder(\n",
        "    root=str(DATASET_ROOT_VAL),\n",
        "    transform=val_transform\n",
        ")\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class_names = train_dataset.classes\n",
        "print(\"clases:\", class_names)\n",
        "\n",
        "train_counts = Counter([label for _, label in train_dataset])\n",
        "print(\"cantidad de imágenes por clase (train):\")\n",
        "for idx, cls in enumerate(class_names):\n",
        "    print(cls, \":\", train_counts[idx])\n"
      ],
      "metadata": {
        "id": "7p8aBrcUgBwK",
        "outputId": "b9f2f42a-7bb1-4c04-b3e0-bcb6207a862d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clases: ['alegria', 'disgusto', 'enojo', 'miedo', 'seriedad', 'sorpresa', 'tristeza']\n",
            "cantidad de imágenes por clase (train):\n",
            "alegria : 4772\n",
            "disgusto : 717\n",
            "enojo : 705\n",
            "miedo : 281\n",
            "seriedad : 2524\n",
            "sorpresa : 1290\n",
            "tristeza : 1982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 1. Extraer los conteos en orden\n",
        "conteos = list(train_counts.values())\n",
        "\n",
        "# 2. Calcular pesos (Inverso de la frecuencia)\n",
        "total_muestras = sum(conteos)\n",
        "class_weights = [total_muestras / c for c in conteos]\n",
        "\n",
        "# Convertir a Tensor\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
        "\n",
        "print(f\"Pesos calculados: {class_weights}\")"
      ],
      "metadata": {
        "id": "tNgqlYRyi4N9",
        "outputId": "637aba88-680f-4859-ad6c-79da0951a28f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pesos calculados: [2.571458507963118, 17.11436541143654, 17.405673758865248, 43.669039145907476, 4.8617274167987325, 9.512403100775193, 6.191220988900101]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado el alto desbalance entre emociones (por ejemplo: alegría 4772 vs miedo 281), se aplicó una pérdida ponderada con pesos inversamente proporcionales a la frecuencia de cada clase. Esto evita que el modelo quede sesgado hacia la clase mayoritaria"
      ],
      "metadata": {
        "id": "YvoxS3A1lVs8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Construcción y entrenamiento del Modelo CNN (3.5 puntos)\n",
        "\n",
        "- Construir una red neuronal convolucional desde cero, sin usar modelos pre-entrenados.\n",
        "- Analizar correctamente qué funciones de activación se deben usar en cada etapa de la red, el learning rate a utilizar, la función de costo y el optimizador.\n",
        "- Cosas como el número de capas, neuronas, número y tamaño de los kernels, entre otros, queda a criterio de ustedes, pero deben estar justificadas."
      ],
      "metadata": {
        "id": "Hk6B2VUvdufx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MiCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MiCNN, self).__init__()\n",
        "\n",
        "        # BLOQUE 1: bordes / texturas simples\n",
        "        # Entrada: 1 canal (gris), Salida: 32\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1   = nn.BatchNorm2d(32)\n",
        "        self.pool  = nn.MaxPool2d(kernel_size=2, stride=2)  # 128 -> 64\n",
        "\n",
        "        # BLOQUE 2: patrones más complejos\n",
        "        # 64 -> 32\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2   = nn.BatchNorm2d(64)\n",
        "\n",
        "        # BLOQUE 3: rasgos de alto nivel (configuración global de la cara)\n",
        "        # 32 -> 16\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3   = nn.BatchNorm2d(128)\n",
        "\n",
        "        # CLASIFICADOR\n",
        "        # 128 canales, mapa final 16x16 (128 -> 64 -> 32 -> 16)\n",
        "        self.flatten_dim = 128 * 16 * 16\n",
        "\n",
        "        self.fc1 = nn.Linear(self.flatten_dim, 128)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "\n",
        "        x = x.view(-1, self.flatten_dim)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "num_clases = len(class_names)\n",
        "model = MiCNN(num_classes=num_clases)\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "AdjPwi7NlEa6",
        "outputId": "c8228b63-1cd0-4f8d-9a97-c8b40e172aac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MiCNN(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=32768, out_features=128, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=128, out_features=7, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Evaluación del Modelo (2.5 puntos)\n",
        "\n",
        "El modelo entrenado debe ser evaluado utilizando las siguientes métricas:\n",
        "\n",
        "- **Accuracy**:\n",
        "  - Reportar el valor final en el conjunto de validación.\n",
        "  - Incluir una gráfica de evolución por época para entrenamiento y validación.\n",
        "\n",
        "- **F1 Score**:\n",
        "  - Reportar el valor final en el conjunto de validación.\n",
        "  - Incluir una gráfica de evolución por época para entrenamiento y validación.\n",
        "\n",
        "- **Costo (Loss)**:\n",
        "  - Mostrar una gráfica de evolución del costo por época para entrenamiento y validación.\n",
        "\n",
        "- **Classification report**\n",
        "  - Mostrar la precisión, recall y F1 score por cada clase usando `classification_report`\n",
        "\n",
        "- **Matriz de confusión**:\n",
        "  - Mostrar la matriz de confusión absoluta (valores enteros).\n",
        "  - Mostrar la matriz de confusión normalizada (valores entre 0 y 1 por fila).\n",
        "\n",
        "Se recomienda utilizar `scikit-learn` para calcular métricas como accuracy, F1 score, el Classification report y las matrices de confusión. Las visualizaciones pueden realizarse con `matplotlib` o `seaborn`, separando claramente los datos de entrenamiento y validación en las gráficas.\n"
      ],
      "metadata": {
        "id": "K5D3EvVRd-Jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else\n",
        "                      \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = MiCNN(num_classes=num_clases).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Para guardar métricas por época\n",
        "train_loss_hist = []\n",
        "val_loss_hist = []\n",
        "train_acc_hist = []\n",
        "val_acc_hist = []\n",
        "train_f1_hist = []\n",
        "val_f1_hist = []\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    train_preds = []\n",
        "    train_labels = []\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        train_preds.extend(preds.cpu().numpy())\n",
        "        train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # métricas de entrenamiento\n",
        "    train_loss = np.mean(train_losses)\n",
        "    train_acc  = accuracy_score(train_labels, train_preds)\n",
        "    train_f1   = f1_score(train_labels, train_preds, average=\"weighted\")\n",
        "\n",
        "    train_loss_hist.append(train_loss)\n",
        "    train_acc_hist.append(train_acc)\n",
        "    train_f1_hist.append(train_f1)\n",
        "\n",
        "    # validación\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_losses.append(loss.item())\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss = np.mean(val_losses)\n",
        "    val_acc  = accuracy_score(val_labels, val_preds)\n",
        "    val_f1   = f1_score(val_labels, val_preds, average=\"weighted\")\n",
        "\n",
        "    val_loss_hist.append(val_loss)\n",
        "    val_acc_hist.append(val_acc)\n",
        "    val_f1_hist.append(val_f1)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} \"\n",
        "          f\"- Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f} \"\n",
        "          f\"- Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f} \"\n",
        "          f\"- Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "60PHRXSCob06",
        "outputId": "74ad9a1a-9d09-4671-d8ee-8991823497b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Train Loss: 2.1795, Val Loss: 1.9165 - Train Acc: 0.3631, Val Acc: 0.3885 - Train F1: 0.2378, Val F1: 0.2260\n",
            "Epoch 2/10 - Train Loss: 1.9135, Val Loss: 1.9035 - Train Acc: 0.3875, Val Acc: 0.3993 - Train F1: 0.2312, Val F1: 0.2478\n",
            "Epoch 3/10 - Train Loss: 1.8968, Val Loss: 1.8951 - Train Acc: 0.3869, Val Acc: 0.3729 - Train F1: 0.2330, Val F1: 0.2361\n",
            "Epoch 4/10 - Train Loss: 1.8980, Val Loss: 1.8819 - Train Acc: 0.3744, Val Acc: 0.3713 - Train F1: 0.2281, Val F1: 0.2442\n",
            "Epoch 5/10 - Train Loss: 1.8917, Val Loss: 1.8825 - Train Acc: 0.3635, Val Acc: 0.2415 - Train F1: 0.2434, Val F1: 0.1418\n",
            "Epoch 6/10 - Train Loss: 1.8806, Val Loss: 1.8681 - Train Acc: 0.2114, Val Acc: 0.2539 - Train F1: 0.0888, Val F1: 0.1399\n",
            "Epoch 7/10 - Train Loss: 1.8930, Val Loss: 1.8665 - Train Acc: 0.2099, Val Acc: 0.2546 - Train F1: 0.0889, Val F1: 0.1475\n",
            "Epoch 8/10 - Train Loss: 1.8831, Val Loss: 1.8683 - Train Acc: 0.2693, Val Acc: 0.2396 - Train F1: 0.2157, Val F1: 0.1073\n",
            "Epoch 9/10 - Train Loss: 1.8803, Val Loss: 1.8701 - Train Acc: 0.2116, Val Acc: 0.2304 - Train F1: 0.0877, Val F1: 0.1204\n",
            "Epoch 10/10 - Train Loss: 1.8778, Val Loss: 1.8555 - Train Acc: 0.2107, Val Acc: 0.2454 - Train F1: 0.0901, Val F1: 0.1218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## 4. Prueba de Imágenes Nuevas (1 punto)\n",
        "Subir al menos 10 imágenes personales de cualquier relación de aspecto (pueden usar fotos del rostro de ustedes, rostros de personas generadas por IA o imágenes stock de internet), que no formen parte del dataset de entrenamiento ni de validación.\n",
        "\n",
        "- Debe haber al menos una imagen para cada emoción.\n",
        "\n",
        "- Aplicar el mismo pre-procesamiento que se usó para el dataset de validation durante el entrenamiento del modelo.\n",
        "\n",
        "- Pasar las imágenes por el modelo entrenado y mostrar:\n",
        "\n",
        "  - La imagen original\n",
        "  - La imagen pre-procesada (mismas transformaciones del entrenamiento)\n",
        "  - El score asignado a cada clase (normalizado de 0 a 1 o de 0% a 100%)\n",
        "  - La clase ganadora inferida por el modelo\n",
        "\n",
        "- Redactar conclusiones preliminares"
      ],
      "metadata": {
        "id": "40tsslqLgFlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## 5. Prueba de Imágenes Nuevas con Pre-procesamiento Adicional (1 punto)\n",
        "Las 10 imágenes del punto 4, ahora serán pasadas y recortadas por un algoritmo de detección de rostros. Usen el siguiente código para realizar un pre-procesamiento inicial de la imagen y ya luego aplican el pre-procesamiento que usaron al momento de entrenar el modelo.\n",
        "\n",
        "- Pasar las imágenes por el modelo entrenado y mostrar:\n",
        "  - La imagen original\n",
        "  - La imagen recortada por el algoritmo\n",
        "  - La imagen pre-procesada (mismas transformaciones del entrenamiento)\n",
        "  - El score asignado a cada clase (normalizado de 0 a 1 o de 0% a 100%)\n",
        "  - La clase ganadora inferida por el modelo\n",
        "\n",
        "- Comparar los resultados con el punto 4 y redactar conclusiones finales.\n",
        "\n",
        "NOTA: Pueden adaptar el código y modificar el `scaleFactor` y el `minNeighbors` según crean conveniente para obtener mejores resultados."
      ],
      "metadata": {
        "id": "w36xM5PrJ4GA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "image_path = \"\"\n",
        "\n",
        "image = cv2.imread(image_path)\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=6)\n",
        "\n",
        "image_with_box = image.copy()\n",
        "for (x, y, w, h) in faces:\n",
        "    cv2.rectangle(image_with_box, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "cropped_face_rgb = None\n",
        "if len(faces) > 0:\n",
        "    (x, y, w, h) = faces[0]\n",
        "    center_x, center_y = x + w // 2, y + h // 2\n",
        "    side = max(w, h)\n",
        "    half_side = side // 2\n",
        "\n",
        "    x1 = max(center_x - half_side, 0)\n",
        "    y1 = max(center_y - half_side, 0)\n",
        "    x2 = min(center_x + half_side, image.shape[1])\n",
        "    y2 = min(center_y + half_side, image.shape[0])\n",
        "\n",
        "    cropped_face = image[y1:y2, x1:x2]\n",
        "    cropped_face_rgb = cv2.cvtColor(cropped_face, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "image_with_box_rgb = cv2.cvtColor(image_with_box, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "ax[0].imshow(image_with_box_rgb)\n",
        "ax[0].set_title(\"Detección\")\n",
        "ax[0].axis('off')\n",
        "\n",
        "if cropped_face_rgb is not None:\n",
        "    ax[1].imshow(cropped_face_rgb)\n",
        "    ax[1].set_title(\"Rostro recortado (relación aspecto 1:1)\")\n",
        "    ax[1].axis('off')\n",
        "else:\n",
        "    ax[1].text(0.5, 0.5, 'No se detectó rostro', horizontalalignment='center', verticalalignment='center')\n",
        "    ax[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gBtAIm1KH2Qm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}